server:
  host: "0.0.0.0"
  port: 8080

logging:
  level: "info"

auth:
  enabled: true
  issuer: "http://keycloak:8080/realms/llm-gateway-realm"
  audience: "llm-gateway-client"

# Defines routing strategies and provider groups
strategies:
  - name: "main_models"
    # Fallback chain: try mockllm
    providers: ["mockllm"] 

# List of all available downstream LLM providers
providers:
  - name: "openai"
    enabled: false
    target_url: "https://api.openai.com"
    api_key: "${OPENAI_API_KEY}"
    timeout: "30s"
    max_retries: 3

  - name: "mockllm"
    enabled: true
    target_url: "http://mockllm:8081"
    api_key: null
    timeout: "60s"
    max_retries: 1
